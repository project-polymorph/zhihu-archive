{
  "id": "1946970482571210761",
  "content": "<p data-pid=\"M25Guz3t\">刚刚，Meta发布了<b>全新开源</b>视觉模型DINOv3——</p><p data-pid=\"x43ctooa\"><b>首次证明</b>了自监督学习模型能够在广泛任务中超越弱监督学习模型。</p><p data-pid=\"r8Uivzvk\">DINOv3采用<b>无标注方法</b>，将数据规模扩展至<b>17亿张图像</b>、模型规模扩展至<b>70亿参数</b>，并能高效支持数据标注稀缺、成本高昂或无法获取的应用场景。</p><p data-pid=\"V_g9QsSH\">DINOv3不仅在缺乏标注或跨领域的场景（网络图像与卫星影像）中表现出绝对的性能领先，还在计算机视觉三大核心任务（分类、检测、分割）上实现了SOTA。</p><p data-pid=\"E3wMXsgn\">网友表示：我还以为你们已经不行了，好在你们终于搞出点东西来了。</p><h2><b>计算机视觉的自监督学习</b></h2><p data-pid=\"6hJQoPQX\">说起计算机视觉，就绕不开李飞飞老师推动的ImageNet和大规模标注数据。</p><p data-pid=\"q7TbGJAL\">然而，随着数据量的激增以及应用场景不断扩展，标注成本和可获取性成为了制约视觉模型通用性的主要因素。</p><p data-pid=\"XweTq8iZ\">基于这一思路，DINOv3采用了创新的自监督学习方法，专注于生成高质量且高分辨率的视觉特征，为下游视觉任务提供强大的骨干模型（backbone）支持。</p><p data-pid=\"DCYUVDBX\">通过这一方法，DINOv3<b>首次实现了单一冻结视觉骨干网络（Single Frozen Vision Backbone）在多项密集预测任务（Dense Prediction Tasks）</b>中超越专门化解决方案的性能。</p><p data-pid=\"uR0COKgr\">那么，DINOv3是怎么做到的？</p><p data-pid=\"k5AWnPxS\">总的来说，DINOv3的训练过程分为两个主要阶段：</p><ul><li data-pid=\"5ZYEgNkW\">DINOv3在一个庞大且精心构建的数据集上进行大规模自监督训练，从而学习到通用且高质量的视觉表示</li><li data-pid=\"sY88JNVm\">引入名为“Gram anchoring”的新方法来解决训练中密集特征图的退化问题，在不影响全局特征的同时，显著提升局部特征的质量</li></ul><p data-pid=\"SXydJoWH\">具体来说，研究者首先构建了一个包含约<b>17亿</b>张图片的预训练数据集。</p><p data-pid=\"1fmeYTky\">这些图片数据主要来自<b>Instagram</b>上的公开图片，以及少量来自<b>ImageNet</b>的图片。</p><p data-pid=\"KvfMNsxt\">在对数据集进行分类、采样后，研究者采用<b>判别式自监督</b>（Discriminative Self-supervised），通过Sinkhorn-Knopp算法和Koleo正则稳定特征分布，实现了细粒度又稳健的密集特征学习。</p><p data-pid=\"xDZibeZB\">此外，在继承DINOv2成功方法的基础上，DINOv3将模型参数<b>从11亿扩展至70亿</b>，以增强骨干网络的表示能力，从而能够从海量图像中学习更丰富、细粒度的视觉特征。</p><p data-pid=\"pu4JVHaW\">相比v2，DINOv3在训练策略上引入了RoPE-box jittering，使模型对<b>分辨率、尺度和长宽比变化更具鲁棒性</b>，同时保留多裁剪训练和恒定学习率+EMA教师动量优化的做法，确保训练稳定且高效。</p><p data-pid=\"tEOcRL20\">在大规模训练中，DINOv3的70亿参数模型可以通过长时间训练显著提升全局任务性能，因此研究者在最初就寄希望于长时间训练。</p><p data-pid=\"vM5QNgf2\">然而，密集预测任务（如图像分割）往往会随着训练迭代次数的增加而下降，而这种退化主要源于patch-level（补丁级别）特征的一致性丧失：</p><blockquote data-pid=\"CQeBRoaZ\">随着训练进行，原本定位良好的patch特征逐渐出现不相关patch与参考patch相似度过高的现象，从而削弱了模型在密集任务中的表现。</blockquote><p data-pid=\"Tnjr1YDc\">为了应对这一问题，研究团队提出了“Gram anchoring”方法，即通过将学生模型的patch Gram矩阵逼近早期训练阶段表现优异的教师模型的Gram矩阵，来保持patch间的相对相似性，而不限制特征本身的自由表达。</p><p data-pid=\"hcXcEZSI\">实验表明，在应用Gram anchoring后，ADE20k分割任务有着显著的提升，且<b>训练稳定性明显增强。</b></p><p data-pid=\"RkzSXj_6\">这表明保持patch-level一致性与学习判别性全局特征之间可以有效协调，而在有针对性的正则化下，长时间训练也不再牺牲密集任务表现。</p><p data-pid=\"q2qMjZaT\">此外，通过将高分辨率图像输入到Gram教师并下采样至与学生输出相同的尺寸，仍然获得了平滑且一致的patch特征图。</p><p data-pid=\"vG5ivbJm\">实验结果显示，即便经过下采样，高分辨率特征中优越的patch-level一致性仍得以保留，从而生成更加平滑、连贯的patch表示。</p><p data-pid=\"ElAVJszh\">最后，由于DINOv3在最初训练时使用了相对较低的分辨率（256×256），为了让模型适应高分辨率的图像场景，研究团队在训练后增加了一个“高分辨率适应步骤”，从而让模型在学会处理更大尺寸图像的同时，还能保持性能稳定。</p><p data-pid=\"LRj63r8j\">在这一适应步骤中，DINOv3结合了<b>“混合分辨率”（mixed resolutions）</b>策略与Gram anchoring方法，使模型在处理更大、更复杂的图像时仍能保持稳定且精细的特征表示，同时兼顾全局任务与密集预测任务的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p> <p data-pid=\"F72aYIx4\">最后，为了验证DINOv3的性能，研究团队在包含密集特征、全局特征任务在内的多个计算机视觉任务上对DINOv3 7B模型进行了评估。</p><p data-pid=\"bfsCI35E\">就像我们在开头提到的，DINOv3在语义分割、单目深度估计、非参数方法、3D对应估计等任务中实现了SOTA。</p><p data-pid=\"2JDpFj4-\">值得一提的是，由于DINOv3强大的通用性，它还消除了研究人员与开发者为了特定任务而对模型进行<b>微调</b>的必要。</p><p data-pid=\"FrZzDuDp\">此外，为了方便社区部署，Meta还通过蒸馏原生的<b>70亿参数模型DINOv3，构建了一个开发环境友好的v3模型矩阵</b>：VisionTransformer(ViT)的Small、Base和Large版本，以及基于ConvNeXt的架构。</p><p data-pid=\"wRSWqpwn\">其中，ViT-H+模型在各种任务上取得了接近原始70亿参数教师模型的性能。</p><figure data-size=\"normal\"><noscript><img src=\"https://pica.zhimg.com/50/v2-71af5f2f9d66bf4ba00f89dcdff74045_720w.jpg?source=1def8aca\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"455\" data-rawheight=\"210\" data-original-token=\"v2-91a3215d29bc025b0719d9ad1c5f8118\" class=\"origin_image zh-lightbox-thumb\" width=\"455\" data-original=\"https://pica.zhimg.com/v2-71af5f2f9d66bf4ba00f89dcdff74045_r.jpg?source=1def8aca\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;455&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"455\" data-rawheight=\"210\" data-original-token=\"v2-91a3215d29bc025b0719d9ad1c5f8118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"455\" data-original=\"https://pica.zhimg.com/v2-71af5f2f9d66bf4ba00f89dcdff74045_r.jpg?source=1def8aca\" data-actualsrc=\"https://pica.zhimg.com/50/v2-71af5f2f9d66bf4ba00f89dcdff74045_720w.jpg?source=1def8aca\"/></figure><p data-pid=\"qyDJOqQ2\">据悉，Meta也透露将发布具体的蒸馏流程，以便社区能够在此基础上继续构建与改进。</p><h2><b>DINO行动</b></h2><p data-pid=\"3fac4Myo\">在实际应用中，DINOv3也展现了强大的泛化能力。</p><p data-pid=\"eYV-DZPx\">例如，在与世界资源研究所（WRI）合作中，Meta利用DINOv3开发了一种算法，能够<b>利用DINOv3分析卫星影像，检测受影响生态系统中的树木损失与土地利用变化。</b>为全球森林恢复和农业管理提供了强有力的技术支持。</p><p data-pid=\"Riva-qDg\">与DINOv2相比，在使用卫星与航空影像进行训练的情况下，DINOv3将肯尼亚某地区树冠高度测量的平均误差从4.1米降低至1.2米。</p><p data-pid=\"aG2X-4NV\">除此此外，DINOv3还在多个遥感任务（包括语义地理空间任务和高分辨率语义任务等）中取得了SOTA。</p><p data-pid=\"EnLKHaTD\">最后，DINO（Distillation With NO Labels）系列作为Meta对视觉领域自监督方法的探索，可以说是一脉相承，继往开来，标志着视觉模型大规模自监督训练的持续进步。</p><p data-pid=\"GoCGyBOa\">从DINO的<b>初步研究概念验证</b>，使用<b>100万</b>张图像训练<b>8000万</b>参数的模型，</p><p data-pid=\"Jna_654l\">到DINOv2中基于<b>1.42</b>亿张图像训练的<b>1B</b>参数模型，SSL算法的首次成功扩展，</p><p data-pid=\"oxNWotjw\">再到如今DINOv3的<b>70亿</b>参数和<b>17亿</b>张图片，</p><p data-pid=\"AZwByRXn\">Meta的这套自监督训练方法有望引领我们迈向更大规模、通用性更强，同时更加精准且高效的视觉理解。</p>",
  "excerpt": "刚刚，Meta发布了 全新开源视觉模型DINOv3—— 首次证明了自监督学习模型能够在广泛任务中超越弱监督学习模型。DINOv3采用 无标注方法，将数据规模扩展至17亿张图像、模型规模扩展至70亿参数，并能高效支持数据标注稀缺、成本高昂或无法获取的应用场景。DINOv3不仅在缺乏标注或跨领域的场景（网络图像与卫星影像）中表现出绝对的性能领先，还在计算机视觉三大核心任务（分类、检测、分割）上实现了SOTA。 网友表示：我还以为你们…",
  "voteupCount": 0,
  "commentCount": 0,
  "favlistsCount": 0,
  "createdTime": "2025-09-04 08:18",
  "updatedTime": "2025-09-04 08:18",
  "author": {
    "id": "297fb57e3532c6e98bd749117eb348c0",
    "name": "量子逐光者",
    "headline": "",
    "avatarUrl": "https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=1def8aca"
  },
  "hotComment": null,
  "questionId": "9392382402",
  "crawledAt": "2026-01-16T19:39:46.123Z"
}